{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h3IuHaV2UUhe",
        "outputId": "ccc7f65a-7ec5-4185-b4c4-25d8c5fe47f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Time-LLM'...\n",
            "remote: Enumerating objects: 189, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 189 (delta 0), reused 1 (delta 0), pack-reused 186 (from 1)\u001b[K\n",
            "Receiving objects: 100% (189/189), 1.09 MiB | 8.49 MiB/s, done.\n",
            "Resolving deltas: 100% (92/92), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/KimMeen/Time-LLM.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd Time-LLM"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fMFtiUlUYKT",
        "outputId": "eb8fa952-2a68-4c81-846a-52551aef6a74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Time-LLM\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Axkb4KhUUZi4",
        "outputId": "4f291d3a-ce57-452e-9607-2ea3dc927b53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==2.2.2 (from -r requirements.txt (line 1))\n",
            "  Downloading torch-2.2.2-cp312-cp312-manylinux1_x86_64.whl.metadata (25 kB)\n",
            "Collecting accelerate==0.28.0 (from -r requirements.txt (line 2))\n",
            "  Downloading accelerate-0.28.0-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting einops==0.7.0 (from -r requirements.txt (line 3))\n",
            "  Downloading einops-0.7.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting matplotlib==3.7.0 (from -r requirements.txt (line 4))\n",
            "  Downloading matplotlib-3.7.0.tar.gz (36.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.3/36.3 MB\u001b[0m \u001b[31m67.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting numpy==1.23.5 (from -r requirements.txt (line 5))\n",
            "  Downloading numpy-1.23.5.tar.gz (10.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.7/10.7 MB\u001b[0m \u001b[31m156.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
            "\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install deepspeed"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dMzJ85nSUbJp",
        "outputId": "58bb23d7-ff6a-498f-9374-6095a717ea99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting deepspeed\n",
            "  Downloading deepspeed-0.18.2.tar.gz (1.6 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.3/1.6 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from deepspeed) (0.8.1)\n",
            "Collecting hjson (from deepspeed)\n",
            "  Downloading hjson-3.1.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.12/dist-packages (from deepspeed) (1.1.2)\n",
            "Collecting ninja (from deepspeed)\n",
            "  Downloading ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from deepspeed) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from deepspeed) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from deepspeed) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.12/dist-packages (from deepspeed) (9.0.0)\n",
            "Requirement already satisfied: pydantic>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from deepspeed) (2.11.10)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from deepspeed) (2.8.0+cu126)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from deepspeed) (4.67.1)\n",
            "Requirement already satisfied: nvidia-ml-py in /usr/local/lib/python3.12/dist-packages (from deepspeed) (13.580.82)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0.0->deepspeed) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0.0->deepspeed) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0.0->deepspeed) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0.0->deepspeed) (0.4.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed) (3.20.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->deepspeed) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->deepspeed) (3.0.3)\n",
            "Downloading hjson-3.1.0-py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (180 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.7/180.7 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: deepspeed\n",
            "  Building wheel for deepspeed (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for deepspeed: filename=deepspeed-0.18.2-py3-none-any.whl size=1763310 sha256=504e551b05ec10cfef61c10564ebd6ccc62da92e33dbf74f71ae21f53cd8e71c\n",
            "  Stored in directory: /root/.cache/pip/wheels/69/ad/2e/e03d4739ddc0417efd8a120c2b9e784005aa226037e558c163\n",
            "Successfully built deepspeed\n",
            "Installing collected packages: hjson, ninja, deepspeed\n",
            "Successfully installed deepspeed-0.18.2 hjson-3.1.0 ninja-1.13.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aq1S74fYVoQR",
        "outputId": "b7414d15-1322-40e1-f56b-42b1ad8a69b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Nov 16 12:10:38 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   31C    P0             47W /  400W |       0MiB /  40960MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install deepspeed accelerate\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ri6o5A7AZ0C4",
        "outputId": "7840bd74-0d5d-4337-fd11-72bf334865ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: deepspeed in /usr/local/lib/python3.12/dist-packages (0.18.2)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.11.0)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from deepspeed) (0.8.1)\n",
            "Requirement already satisfied: hjson in /usr/local/lib/python3.12/dist-packages (from deepspeed) (3.1.0)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.12/dist-packages (from deepspeed) (1.1.2)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.12/dist-packages (from deepspeed) (1.13.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from deepspeed) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from deepspeed) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from deepspeed) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.12/dist-packages (from deepspeed) (9.0.0)\n",
            "Requirement already satisfied: pydantic>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from deepspeed) (2.11.10)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from deepspeed) (2.8.0+cu126)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from deepspeed) (4.67.1)\n",
            "Requirement already satisfied: nvidia-ml-py in /usr/local/lib/python3.12/dist-packages (from deepspeed) (13.580.82)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate) (6.0.3)\n",
            "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (0.36.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate) (0.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2025.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2.32.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0.0->deepspeed) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0.0->deepspeed) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0.0->deepspeed) (0.4.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->deepspeed) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->deepspeed) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2025.10.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Set the environment variable for GPU usage\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "\n",
        "# Then proceed with the rest of the configuration and shell script execution\n",
        "num_process = 1\n",
        "master_port = 97\n",
        "batch_size = 24\n",
        "d_model = 32\n",
        "d_ff = 128\n",
        "learning_rate = 0.01\n",
        "train_epochs = 100\n",
        "llama_layers = 32\n",
        "comment = 'TimeLLM-ETTh1'\n",
        "\n",
        "# Run the shell script with the configured variables\n",
        "!num_process=1 \\\n",
        "  master_port=97 \\\n",
        "  batch_size=24 \\\n",
        "  d_model=32 \\\n",
        "  d_ff=128 \\\n",
        "  learning_rate=0.01 \\\n",
        "  train_epochs=100 \\\n",
        "  llama_layers=32 \\\n",
        "  comment='TimeLLM-ETTh1' \\\n",
        "  bash scripts/TimeLLM_ETTh1.sh\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9w5YkrUa7Sy",
        "outputId": "8d32e7b0-e604-465f-9ed1-a2f9fe7a0798"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
            "\t`--num_machines` was set to a value of `1`\n",
            "\t`--dynamo_backend` was set to a value of `'no'`\n",
            "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
            "2025-11-16 12:14:55.717770: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-16 12:14:55.735834: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-11-16 12:14:55.757598: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763295295.757598   10006 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763295295.764197   10006 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763295295.780756   10006 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763295295.780791   10006 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763295295.780794   10006 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763295295.780797   10006 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-16 12:14:55.781676: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-11-16 12:14:55.785503: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763295295.814353   10000 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763295295.824207   10000 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763295295.847347   10000 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763295295.847386   10000 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763295295.847391   10000 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763295295.847395   10000 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-16 12:14:55.854258: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-11-16 12:14:55.932541: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-16 12:14:55.958688: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763295295.993815   10001 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763295296.005103   10001 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763295296.032829   10001 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763295296.032878   10001 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763295296.032885   10001 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763295296.032891   10001 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-16 12:14:56.040963: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-11-16 12:14:56.679460: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-16 12:14:56.705216: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-11-16 12:14:56.725750: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763295296.739392   10005 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-11-16 12:14:56.742209: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "E0000 00:00:1763295296.749875   10005 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763295296.763062   10003 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763295296.769384   10003 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763295296.774451   10005 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763295296.774492   10005 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763295296.774497   10005 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763295296.774502   10005 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-16 12:14:56.781836: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "W0000 00:00:1763295296.784802   10003 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763295296.784829   10003 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763295296.784832   10003 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763295296.784834   10003 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-16 12:14:56.789491: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-11-16 12:14:57.203716: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-16 12:14:57.221180: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763295297.242427   10002 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763295297.248771   10002 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763295297.264259   10002 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763295297.264291   10002 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763295297.264295   10002 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763295297.264299   10002 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-16 12:14:57.268915: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-11-16 12:14:57.522294: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-16 12:14:57.539356: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763295297.561126    9999 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763295297.567709    9999 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763295297.584597    9999 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763295297.584634    9999 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763295297.584637    9999 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763295297.584639    9999 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-16 12:14:57.589489: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-11-16 12:14:58.212266: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-16 12:14:58.239247: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763295298.275082   10004 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763295298.286089   10004 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763295298.311519   10004 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763295298.311575   10004 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763295298.311580   10004 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763295298.311585   10004 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-16 12:14:58.319005: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Time-LLM/run_main.py\", line 104, in <module>\n",
            "    accelerator = Accelerator(kwargs_handlers=[ddp_kwargs], deepspeed_plugin=deepspeed_plugin)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py\", line 461, in __init__\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Time-LLM/run_main.py\", line 104, in <module>\n",
            "    self.state = AcceleratorState(\n",
            "             accelerator = Accelerator(kwargs_handlers=[ddp_kwargs], deepspeed_plugin=deepspeed_plugin)\n",
            "        ^  ^  ^   ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/state.py\", line 912, in __init__\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Time-LLM/run_main.py\", line 104, in <module>\n",
            "Traceback (most recent call last):\n",
            "    accelerator = Accelerator(kwargs_handlers=[ddp_kwargs], deepspeed_plugin=deepspeed_plugin)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py\", line 461, in __init__\n",
            "Traceback (most recent call last):\n",
            "    self.state = AcceleratorState(\n",
            "  File \"/content/Time-LLM/run_main.py\", line 104, in <module>\n",
            "                 ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/state.py\", line 912, in __init__\n",
            "    accelerator = Accelerator(kwargs_handlers=[ddp_kwargs], deepspeed_plugin=deepspeed_plugin)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^    PartialState(cpu, **kwargs)\n",
            "^^^^^^^  File \"/usr/local/lib/python3.12/dist-packages/accelerate/state.py\", line 216, in __init__\n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^    dist.init_distributed(dist_backend=self.backend, auto_mpi_discovery=False, **kwargs)\n",
            "           ^^^^  File \"/usr/local/lib/python3.12/dist-packages/deepspeed/comm/comm.py\", line 854, in init_distributed\n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py\", line 461, in __init__\n",
            "    cdb = TorchBackend(dist_backend, timeout, init_method, rank, world_size)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^    ^self.state = AcceleratorState(^\n",
            "^^^^^^^^^^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^^^^^^^^^^^^^^^^^^\n",
            "^^^^^  File \"/usr/local/lib/python3.12/dist-packages/deepspeed/comm/torch.py\", line 120, in __init__\n",
            "^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/state.py\", line 912, in __init__\n",
            "    self.init_process_group(backend, timeout, init_method, rank, world_size)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/deepspeed/comm/torch.py\", line 160, in init_process_group\n",
            "    torch.distributed.init_process_group(backend, **kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/c10d_logger.py\", line 81, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "             PartialState(cpu, **kwargs) \n",
            " ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/c10d_logger.py\", line 95, in wrapper\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/state.py\", line 216, in __init__\n",
            "    func_return = func(*args, **kwargs)\n",
            "                  ^^^^^^^^^^^^^^^^^^^    ^dist.init_distributed(dist_backend=self.backend, auto_mpi_discovery=False, **kwargs)^\n",
            "\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py\", line 1697, in init_process_group\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/deepspeed/comm/comm.py\", line 854, in init_distributed\n",
            "^^    PartialState(cpu, **kwargs)^\n",
            "^^^^^^^^^^^  File \"/usr/local/lib/python3.12/dist-packages/accelerate/state.py\", line 216, in __init__\n",
            "^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py\", line 461, in __init__\n",
            "    dist.init_distributed(dist_backend=self.backend, auto_mpi_discovery=False, **kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/deepspeed/comm/comm.py\", line 854, in init_distributed\n",
            "    raise ValueError(\n",
            "ValueError: device_id cuda:7 is out of range. Please use a device index less than the number of accelerators available: 1.\n",
            "    self.state = AcceleratorState(\n",
            "                 ^^^^^^^^^^^^    ^cdb = TorchBackend(dist_backend, timeout, init_method, rank, world_size)^\n",
            "^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/state.py\", line 912, in __init__\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^      File \"/content/Time-LLM/run_main.py\", line 104, in <module>\n",
            "^PartialState(cpu, **kwargs)^\n",
            "^^^^^^^  File \"/usr/local/lib/python3.12/dist-packages/accelerate/state.py\", line 216, in __init__\n",
            "^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/deepspeed/comm/torch.py\", line 120, in __init__\n",
            "    accelerator = Accelerator(kwargs_handlers=[ddp_kwargs], deepspeed_plugin=deepspeed_plugin)\n",
            "    dist.init_distributed(dist_backend=self.backend, auto_mpi_discovery=False, **kwargs)\n",
            "    self.init_process_group(backend, timeout, init_method, rank, world_size) \n",
            "            File \"/usr/local/lib/python3.12/dist-packages/deepspeed/comm/comm.py\", line 854, in init_distributed\n",
            "cdb = TorchBackend(dist_backend, timeout, init_method, rank, world_size) \n",
            "   File \"/usr/local/lib/python3.12/dist-packages/deepspeed/comm/torch.py\", line 160, in init_process_group\n",
            "            ^ ^ ^ ^ ^ ^ ^ ^    ^^torch.distributed.init_process_group(backend, **kwargs)^^\n",
            "^^^^^^^^^^^^^^  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/c10d_logger.py\", line 81, in wrapper\n",
            "^^^^^^^^^^^^^^^^^^^    ^^cdb = TorchBackend(dist_backend, timeout, init_method, rank, world_size)^^\n",
            "^^    ^^return func(*args, **kwargs)^^\n",
            "^^^^^ ^^ ^^ ^^  ^^  ^^  ^^  ^^  ^^  ^^  ^^ ^^^ ^^^ ^^^^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "^^^^^^^^^\n",
            "^  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/c10d_logger.py\", line 95, in wrapper\n",
            "^^^^^^^^  File \"/usr/local/lib/python3.12/dist-packages/deepspeed/comm/torch.py\", line 120, in __init__\n",
            "^\n",
            "^^^^^^^^^^^^^  File \"/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py\", line 461, in __init__\n",
            "^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/deepspeed/comm/torch.py\", line 120, in __init__\n",
            "    func_return = func(*args, **kwargs)\n",
            "    self.init_process_group(backend, timeout, init_method, rank, world_size)\n",
            "                File \"/usr/local/lib/python3.12/dist-packages/deepspeed/comm/torch.py\", line 160, in init_process_group\n",
            "    ^^^^^^^^^^^^^^    ^self.init_process_group(backend, timeout, init_method, rank, world_size)^\n",
            "^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py\", line 1697, in init_process_group\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/deepspeed/comm/torch.py\", line 160, in init_process_group\n",
            "    torch.distributed.init_process_group(backend, **kwargs)\n",
            "    self.state = AcceleratorState(\n",
            "    File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/c10d_logger.py\", line 81, in wrapper\n",
            "               ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/state.py\", line 912, in __init__\n",
            "    torch.distributed.init_process_group(backend, **kwargs)\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/c10d_logger.py\", line 81, in wrapper\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/c10d_logger.py\", line 95, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "      func_return = func(*args, **kwargs) \n",
            "        ^^^^^^^^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ \n",
            "    ^  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/c10d_logger.py\", line 95, in wrapper\n",
            "^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py\", line 1697, in init_process_group\n",
            "    PartialState(cpu, **kwargs)\n",
            "    func_return = func(*args, **kwargs)\n",
            "     File \"/usr/local/lib/python3.12/dist-packages/accelerate/state.py\", line 216, in __init__\n",
            "              raise ValueError( \n",
            "    ^^^^ValueError^^: ^device_id cuda:2 is out of range. Please use a device index less than the number of accelerators available: 1.^\n",
            "^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py\", line 1697, in init_process_group\n",
            "    dist.init_distributed(dist_backend=self.backend, auto_mpi_discovery=False, **kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/deepspeed/comm/comm.py\", line 854, in init_distributed\n",
            "    raise ValueError(\n",
            "ValueError: device_id cuda:6 is out of range. Please use a device index less than the number of accelerators available: 1.\n",
            "    cdb = TorchBackend(dist_backend, timeout, init_method, rank, world_size)\n",
            "       raise ValueError( \n",
            "   ValueError :  device_id cuda:3 is out of range. Please use a device index less than the number of accelerators available: 1. ^^^\n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^Traceback (most recent call last):\n",
            "^^^^^  File \"/content/Time-LLM/run_main.py\", line 104, in <module>\n",
            "^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/deepspeed/comm/torch.py\", line 120, in __init__\n",
            "    accelerator = Accelerator(kwargs_handlers=[ddp_kwargs], deepspeed_plugin=deepspeed_plugin)\n",
            "    self.init_process_group(backend, timeout, init_method, rank, world_size)\n",
            "              File \"/usr/local/lib/python3.12/dist-packages/deepspeed/comm/torch.py\", line 160, in init_process_group\n",
            "      ^^^^^^^^^^^^^^^^^^^^^    ^torch.distributed.init_process_group(backend, **kwargs)^\n",
            "^^^^^^^^^^^^  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/c10d_logger.py\", line 81, in wrapper\n",
            "^^^^^^^^^^^^    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/c10d_logger.py\", line 95, in wrapper\n",
            "    func_return = func(*args, **kwargs)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py\", line 1697, in init_process_group\n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py\", line 461, in __init__\n",
            "    self.state = AcceleratorState(\n",
            "                 ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/state.py\", line 912, in __init__\n",
            "    PartialState(cpu, **kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/state.py\", line 216, in __init__\n",
            "    raise ValueError(\n",
            "ValueError: device_id cuda:4 is out of range. Please use a device index less than the number of accelerators available: 1.\n",
            "    dist.init_distributed(dist_backend=self.backend, auto_mpi_discovery=False, **kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/deepspeed/comm/comm.py\", line 854, in init_distributed\n",
            "    cdb = TorchBackend(dist_backend, timeout, init_method, rank, world_size)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/deepspeed/comm/torch.py\", line 120, in __init__\n",
            "    self.init_process_group(backend, timeout, init_method, rank, world_size)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/deepspeed/comm/torch.py\", line 160, in init_process_group\n",
            "    torch.distributed.init_process_group(backend, **kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/c10d_logger.py\", line 81, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/c10d_logger.py\", line 95, in wrapper\n",
            "    func_return = func(*args, **kwargs)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py\", line 1697, in init_process_group\n",
            "    raise ValueError(\n",
            "ValueError: device_id cuda:1 is out of range. Please use a device index less than the number of accelerators available: 1.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Time-LLM/run_main.py\", line 104, in <module>\n",
            "    accelerator = Accelerator(kwargs_handlers=[ddp_kwargs], deepspeed_plugin=deepspeed_plugin)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py\", line 461, in __init__\n",
            "    self.state = AcceleratorState(\n",
            "                 ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/state.py\", line 912, in __init__\n",
            "    PartialState(cpu, **kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/state.py\", line 216, in __init__\n",
            "    dist.init_distributed(dist_backend=self.backend, auto_mpi_discovery=False, **kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/deepspeed/comm/comm.py\", line 854, in init_distributed\n",
            "    cdb = TorchBackend(dist_backend, timeout, init_method, rank, world_size)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/deepspeed/comm/torch.py\", line 120, in __init__\n",
            "    self.init_process_group(backend, timeout, init_method, rank, world_size)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/deepspeed/comm/torch.py\", line 160, in init_process_group\n",
            "    torch.distributed.init_process_group(backend, **kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/c10d_logger.py\", line 81, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/c10d_logger.py\", line 95, in wrapper\n",
            "    func_return = func(*args, **kwargs)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py\", line 1697, in init_process_group\n",
            "    raise ValueError(\n",
            "ValueError: device_id cuda:5 is out of range. Please use a device index less than the number of accelerators available: 1.\n",
            "W1116 12:15:15.652000 9872 torch/distributed/elastic/multiprocessing/api.py:900] Sending process 9999 closing signal SIGTERM\n",
            "W1116 12:15:15.653000 9872 torch/distributed/elastic/multiprocessing/api.py:900] Sending process 10000 closing signal SIGTERM\n",
            "W1116 12:15:15.654000 9872 torch/distributed/elastic/multiprocessing/api.py:900] Sending process 10003 closing signal SIGTERM\n",
            "W1116 12:15:15.655000 9872 torch/distributed/elastic/multiprocessing/api.py:900] Sending process 10004 closing signal SIGTERM\n",
            "W1116 12:15:15.659000 9872 torch/distributed/elastic/multiprocessing/api.py:900] Sending process 10005 closing signal SIGTERM\n",
            "E1116 12:15:15.824000 9872 torch/distributed/elastic/multiprocessing/api.py:874] failed (exitcode: 1) local_rank: 2 (pid: 10001) of binary: /usr/bin/python3\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/accelerate\", line 10, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/commands/accelerate_cli.py\", line 50, in main\n",
            "    args.func(args)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/commands/launch.py\", line 1226, in launch_command\n",
            "    multi_gpu_launcher(args)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/commands/launch.py\", line 853, in multi_gpu_launcher\n",
            "    distrib_run.run(args)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/run.py\", line 892, in run\n",
            "    elastic_launch(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/launcher/api.py\", line 143, in __call__\n",
            "    return launch_agent(self._config, self._entrypoint, list(args))\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/launcher/api.py\", line 277, in launch_agent\n",
            "    raise ChildFailedError(\n",
            "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
            "============================================================\n",
            "run_main.py FAILED\n",
            "------------------------------------------------------------\n",
            "Failures:\n",
            "[1]:\n",
            "  time      : 2025-11-16_12:15:15\n",
            "  host      : bb8905d3fe8a\n",
            "  rank      : 3 (local_rank: 3)\n",
            "  exitcode  : 1 (pid: 10002)\n",
            "  error_file: <N/A>\n",
            "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
            "------------------------------------------------------------\n",
            "Root Cause (first observed failure):\n",
            "[0]:\n",
            "  time      : 2025-11-16_12:15:15\n",
            "  host      : bb8905d3fe8a\n",
            "  rank      : 2 (local_rank: 2)\n",
            "  exitcode  : 1 (pid: 10001)\n",
            "  error_file: <N/A>\n",
            "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
            "============================================================\n",
            "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
            "\t`--num_machines` was set to a value of `1`\n",
            "\t`--dynamo_backend` was set to a value of `'no'`\n",
            "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
            "2025-11-16 12:15:39.147851: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-16 12:15:39.168562: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-11-16 12:15:39.169070: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-16 12:15:39.191143: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763295339.191684   10767 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-11-16 12:15:39.196483: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "E0000 00:00:1763295339.199926   10767 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-11-16 12:15:39.213560: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763295339.214621   10769 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "W0000 00:00:1763295339.216276   10767 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763295339.216306   10767 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763295339.216310   10767 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763295339.216314   10767 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "E0000 00:00:1763295339.221073   10769 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-11-16 12:15:39.221096: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "W0000 00:00:1763295339.236776   10769 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763295339.236808   10769 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763295339.236811   10769 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763295339.236813   10769 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763295339.239167   10764 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-11-16 12:15:39.241413: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "E0000 00:00:1763295339.245676   10764 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763295339.263197   10764 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763295339.263226   10764 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763295339.263230   10764 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763295339.263234   10764 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-16 12:15:39.268028: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-11-16 12:15:39.520899: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-16 12:15:39.547069: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763295339.568661   10770 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763295339.575063   10770 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763295339.590809   10770 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763295339.590842   10770 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763295339.590845   10770 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763295339.590848   10770 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-16 12:15:39.595653: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-11-16 12:15:41.383049: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-16 12:15:41.401620: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763295341.422906   10766 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763295341.429297   10766 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763295341.446050   10766 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763295341.446087   10766 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763295341.446090   10766 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763295341.446093   10766 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-16 12:15:41.450946: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-11-16 12:15:42.381162: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-16 12:15:42.381347: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-16 12:15:42.381700: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-16 12:15:42.408045: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-11-16 12:15:42.408089: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-11-16 12:15:42.408089: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763295342.443684   10765 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763295342.444339   10768 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763295342.444339   10763 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763295342.454584   10765 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "E0000 00:00:1763295342.455418   10763 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "E0000 00:00:1763295342.455418   10768 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763295342.479658   10765 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763295342.479696   10765 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763295342.479701   10765 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763295342.479704   10765 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763295342.480677   10768 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763295342.480677   10763 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763295342.480709   10768 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763295342.480711   10763 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763295342.480714   10768 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763295342.480716   10763 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763295342.480718   10768 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763295342.480720   10763 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-16 12:15:42.486971: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-11-16 12:15:42.487870: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-11-16 12:15:42.487870: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Time-LLM/run_main.py\", line 104, in <module>\n",
            "    accelerator = Accelerator(kwargs_handlers=[ddp_kwargs], deepspeed_plugin=deepspeed_plugin)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py\", line 461, in __init__\n",
            "    self.state = AcceleratorState(\n",
            "                 ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/state.py\", line 912, in __init__\n",
            "    PartialState(cpu, **kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/state.py\", line 216, in __init__\n",
            "    dist.init_distributed(dist_backend=self.backend, auto_mpi_discovery=False, **kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/deepspeed/comm/comm.py\", line 854, in init_distributed\n",
            "    cdb = TorchBackend(dist_backend, timeout, init_method, rank, world_size)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/deepspeed/comm/torch.py\", line 120, in __init__\n",
            "    self.init_process_group(backend, timeout, init_method, rank, world_size)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/deepspeed/comm/torch.py\", line 160, in init_process_group\n",
            "    torch.distributed.init_process_group(backend, **kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/c10d_logger.py\", line 81, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/c10d_logger.py\", line 95, in wrapper\n",
            "    func_return = func(*args, **kwargs)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py\", line 1697, in init_process_group\n",
            "    raise ValueError(\n",
            "ValueError: device_id cuda:1 is out of range. Please use a device index less than the number of accelerators available: 1.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Time-LLM/run_main.py\", line 104, in <module>\n",
            "    accelerator = Accelerator(kwargs_handlers=[ddp_kwargs], deepspeed_plugin=deepspeed_plugin)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py\", line 461, in __init__\n",
            "    self.state = AcceleratorState(\n",
            "                 ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/state.py\", line 912, in __init__\n",
            "    PartialState(cpu, **kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/state.py\", line 216, in __init__\n",
            "    dist.init_distributed(dist_backend=self.backend, auto_mpi_discovery=False, **kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/deepspeed/comm/comm.py\", line 854, in init_distributed\n",
            "    cdb = TorchBackend(dist_backend, timeout, init_method, rank, world_size)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/deepspeed/comm/torch.py\", line 120, in __init__\n",
            "    self.init_process_group(backend, timeout, init_method, rank, world_size)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/deepspeed/comm/torch.py\", line 160, in init_process_group\n",
            "    torch.distributed.init_process_group(backend, **kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/c10d_logger.py\", line 81, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/c10d_logger.py\", line 95, in wrapper\n",
            "    func_return = func(*args, **kwargs)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py\", line 1697, in init_process_group\n",
            "    raise ValueError(\n",
            "ValueError: device_id cuda:6 is out of range. Please use a device index less than the number of accelerators available: 1.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Time-LLM/run_main.py\", line 104, in <module>\n",
            "    accelerator = Accelerator(kwargs_handlers=[ddp_kwargs], deepspeed_plugin=deepspeed_plugin)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py\", line 461, in __init__\n",
            "    self.state = AcceleratorState(\n",
            "                 ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/state.py\", line 912, in __init__\n",
            "    PartialState(cpu, **kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/state.py\", line 216, in __init__\n",
            "    dist.init_distributed(dist_backend=self.backend, auto_mpi_discovery=False, **kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/deepspeed/comm/comm.py\", line 854, in init_distributed\n",
            "    cdb = TorchBackend(dist_backend, timeout, init_method, rank, world_size)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/deepspeed/comm/torch.py\", line 120, in __init__\n",
            "    self.init_process_group(backend, timeout, init_method, rank, world_size)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/deepspeed/comm/torch.py\", line 160, in init_process_group\n",
            "    torch.distributed.init_process_group(backend, **kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/c10d_logger.py\", line 81, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/c10d_logger.py\", line 95, in wrapper\n",
            "    func_return = func(*args, **kwargs)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py\", line 1697, in init_process_group\n",
            "    raise ValueError(\n",
            "ValueError: device_id cuda:4 is out of range. Please use a device index less than the number of accelerators available: 1.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Time-LLM/run_main.py\", line 104, in <module>\n",
            "    accelerator = Accelerator(kwargs_handlers=[ddp_kwargs], deepspeed_plugin=deepspeed_plugin)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py\", line 461, in __init__\n",
            "    self.state = AcceleratorState(\n",
            "                 ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/state.py\", line 912, in __init__\n",
            "    PartialState(cpu, **kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/state.py\", line 216, in __init__\n",
            "    dist.init_distributed(dist_backend=self.backend, auto_mpi_discovery=False, **kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/deepspeed/comm/comm.py\", line 854, in init_distributed\n",
            "    cdb = TorchBackend(dist_backend, timeout, init_method, rank, world_size)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/deepspeed/comm/torch.py\", line 120, in __init__\n",
            "    self.init_process_group(backend, timeout, init_method, rank, world_size)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/deepspeed/comm/torch.py\", line 160, in init_process_group\n",
            "    torch.distributed.init_process_group(backend, **kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/c10d_logger.py\", line 81, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/c10d_logger.py\", line 95, in wrapper\n",
            "    func_return = func(*args, **kwargs)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py\", line 1697, in init_process_group\n",
            "    raise ValueError(\n",
            "ValueError: device_id cuda:3 is out of range. Please use a device index less than the number of accelerators available: 1.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Time-LLM/run_main.py\", line 104, in <module>\n",
            "    accelerator = Accelerator(kwargs_handlers=[ddp_kwargs], deepspeed_plugin=deepspeed_plugin)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py\", line 461, in __init__\n",
            "    self.state = AcceleratorState(\n",
            "                 ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/state.py\", line 912, in __init__\n",
            "    PartialState(cpu, **kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/state.py\", line 216, in __init__\n",
            "    dist.init_distributed(dist_backend=self.backend, auto_mpi_discovery=False, **kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/deepspeed/comm/comm.py\", line 854, in init_distributed\n",
            "    cdb = TorchBackend(dist_backend, timeout, init_method, rank, world_size)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/deepspeed/comm/torch.py\", line 120, in __init__\n",
            "    self.init_process_group(backend, timeout, init_method, rank, world_size)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/deepspeed/comm/torch.py\", line 160, in init_process_group\n",
            "    torch.distributed.init_process_group(backend, **kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/c10d_logger.py\", line 81, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/c10d_logger.py\", line 95, in wrapper\n",
            "    func_return = func(*args, **kwargs)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py\", line 1697, in init_process_group\n",
            "    raise ValueError(\n",
            "ValueError: device_id cuda:7 is out of range. Please use a device index less than the number of accelerators available: 1.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Time-LLM/run_main.py\", line 104, in <module>\n",
            "    accelerator = Accelerator(kwargs_handlers=[ddp_kwargs], deepspeed_plugin=deepspeed_plugin)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py\", line 461, in __init__\n",
            "    self.state = AcceleratorState(\n",
            "                 ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/state.py\", line 912, in __init__\n",
            "    PartialState(cpu, **kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/state.py\", line 216, in __init__\n",
            "    dist.init_distributed(dist_backend=self.backend, auto_mpi_discovery=False, **kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/deepspeed/comm/comm.py\", line 854, in init_distributed\n",
            "    cdb = TorchBackend(dist_backend, timeout, init_method, rank, world_size)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/deepspeed/comm/torch.py\", line 120, in __init__\n",
            "    self.init_process_group(backend, timeout, init_method, rank, world_size)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/deepspeed/comm/torch.py\", line 160, in init_process_group\n",
            "    torch.distributed.init_process_group(backend, **kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/c10d_logger.py\", line 81, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/c10d_logger.py\", line 95, in wrapper\n",
            "    func_return = func(*args, **kwargs)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py\", line 1697, in init_process_group\n",
            "    raise ValueError(\n",
            "ValueError: device_id cuda:5 is out of range. Please use a device index less than the number of accelerators available: 1.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Time-LLM/run_main.py\", line 104, in <module>\n",
            "    accelerator = Accelerator(kwargs_handlers=[ddp_kwargs], deepspeed_plugin=deepspeed_plugin)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py\", line 461, in __init__\n",
            "    self.state = AcceleratorState(\n",
            "                 ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/state.py\", line 912, in __init__\n",
            "    PartialState(cpu, **kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/state.py\", line 216, in __init__\n",
            "    dist.init_distributed(dist_backend=self.backend, auto_mpi_discovery=False, **kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/deepspeed/comm/comm.py\", line 854, in init_distributed\n",
            "    cdb = TorchBackend(dist_backend, timeout, init_method, rank, world_size)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/deepspeed/comm/torch.py\", line 120, in __init__\n",
            "    self.init_process_group(backend, timeout, init_method, rank, world_size)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/deepspeed/comm/torch.py\", line 160, in init_process_group\n",
            "    torch.distributed.init_process_group(backend, **kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/c10d_logger.py\", line 81, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/c10d_logger.py\", line 95, in wrapper\n",
            "    func_return = func(*args, **kwargs)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py\", line 1697, in init_process_group\n",
            "    raise ValueError(\n",
            "ValueError: device_id cuda:2 is out of range. Please use a device index less than the number of accelerators available: 1.\n",
            "W1116 12:15:54.109000 10656 torch/distributed/elastic/multiprocessing/api.py:900] Sending process 10763 closing signal SIGTERM\n",
            "W1116 12:15:54.110000 10656 torch/distributed/elastic/multiprocessing/api.py:900] Sending process 10764 closing signal SIGTERM\n",
            "W1116 12:15:54.111000 10656 torch/distributed/elastic/multiprocessing/api.py:900] Sending process 10765 closing signal SIGTERM\n",
            "W1116 12:15:54.112000 10656 torch/distributed/elastic/multiprocessing/api.py:900] Sending process 10766 closing signal SIGTERM\n",
            "W1116 12:15:54.113000 10656 torch/distributed/elastic/multiprocessing/api.py:900] Sending process 10767 closing signal SIGTERM\n",
            "W1116 12:15:54.114000 10656 torch/distributed/elastic/multiprocessing/api.py:900] Sending process 10768 closing signal SIGTERM\n",
            "W1116 12:15:54.115000 10656 torch/distributed/elastic/multiprocessing/api.py:900] Sending process 10770 closing signal SIGTERM\n",
            "E1116 12:15:54.335000 10656 torch/distributed/elastic/multiprocessing/api.py:874] failed (exitcode: 1) local_rank: 6 (pid: 10769) of binary: /usr/bin/python3\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/accelerate\", line 10, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/commands/accelerate_cli.py\", line 50, in main\n",
            "    args.func(args)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/commands/launch.py\", line 1226, in launch_command\n",
            "    multi_gpu_launcher(args)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/commands/launch.py\", line 853, in multi_gpu_launcher\n",
            "    distrib_run.run(args)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/run.py\", line 892, in run\n",
            "    elastic_launch(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/launcher/api.py\", line 143, in __call__\n",
            "    return launch_agent(self._config, self._entrypoint, list(args))\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/launcher/api.py\", line 277, in launch_agent\n",
            "    raise ChildFailedError(\n",
            "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
            "============================================================\n",
            "run_main.py FAILED\n",
            "------------------------------------------------------------\n",
            "Failures:\n",
            "  <NO_OTHER_FAILURES>\n",
            "------------------------------------------------------------\n",
            "Root Cause (first observed failure):\n",
            "[0]:\n",
            "  time      : 2025-11-16_12:15:54\n",
            "  host      : bb8905d3fe8a\n",
            "  rank      : 6 (local_rank: 6)\n",
            "  exitcode  : 1 (pid: 10769)\n",
            "  error_file: <N/A>\n",
            "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
            "============================================================\n",
            "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
            "\t`--num_machines` was set to a value of `1`\n",
            "\t`--dynamo_backend` was set to a value of `'no'`\n",
            "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
            "2025-11-16 12:16:17.730036: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-16 12:16:17.746647: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-16 12:16:17.747694: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-11-16 12:16:17.763403: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763295377.769484   11348 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763295377.776069   11348 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763295377.784993   11349 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763295377.791443   11349 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763295377.792611   11348 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763295377.792642   11348 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763295377.792646   11348 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763295377.792650   11348 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-16 12:16:17.792770: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-16 12:16:17.797566: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "W0000 00:00:1763295377.806996   11349 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763295377.807026   11349 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763295377.807029   11349 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763295377.807031   11349 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-16 12:16:17.811706: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-11-16 12:16:17.818622: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763295377.853397   11352 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763295377.864001   11352 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763295377.889054   11352 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763295377.889093   11352 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763295377.889097   11352 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763295377.889101   11352 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-16 12:16:17.896265: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-11-16 12:16:17.989315: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-16 12:16:18.007226: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763295378.028944   11350 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763295378.035660   11350 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763295378.051465   11350 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763295378.051499   11350 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763295378.051501   11350 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763295378.051504   11350 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-16 12:16:18.056296: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-11-16 12:16:20.151825: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-16 12:16:20.168738: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763295380.190757   11353 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763295380.197377   11353 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763295380.213479   11353 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763295380.213512   11353 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763295380.213516   11353 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763295380.213518   11353 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-16 12:16:20.218360: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-11-16 12:16:20.642088: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-16 12:16:20.667832: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763295380.701761   11354 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763295380.712087   11354 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763295380.735980   11354 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763295380.736020   11354 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763295380.736025   11354 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763295380.736028   11354 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-16 12:16:20.742830: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-11-16 12:16:20.909734: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-16 12:16:20.938458: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-11-16 12:16:20.954360: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763295380.977169   11351 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-11-16 12:16:20.981308: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "E0000 00:00:1763295380.988264   11351 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763295381.014658   11351 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763295381.014699   11351 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763295381.014705   11351 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763295381.014710   11351 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763295381.017119   11355 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-11-16 12:16:21.021821: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "E0000 00:00:1763295381.028080   11355 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763295381.053759   11355 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763295381.053801   11355 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763295381.053807   11355 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763295381.053811   11355 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-16 12:16:21.061617: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Time-LLM/run_main.py\", line 104, in <module>\n",
            "    accelerator = Accelerator(kwargs_handlers=[ddp_kwargs], deepspeed_plugin=deepspeed_plugin)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py\", line 461, in __init__\n",
            "    self.state = AcceleratorState(\n",
            "                 ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/state.py\", line 912, in __init__\n",
            "    PartialState(cpu, **kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/state.py\", line 216, in __init__\n",
            "    dist.init_distributed(dist_backend=self.backend, auto_mpi_discovery=False, **kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/deepspeed/comm/comm.py\", line 854, in init_distributed\n",
            "    cdb = TorchBackend(dist_backend, timeout, init_method, rank, world_size)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/deepspeed/comm/torch.py\", line 120, in __init__\n",
            "    self.init_process_group(backend, timeout, init_method, rank, world_size)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/deepspeed/comm/torch.py\", line 160, in init_process_group\n",
            "    torch.distributed.init_process_group(backend, **kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/c10d_logger.py\", line 81, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/c10d_logger.py\", line 95, in wrapper\n",
            "    func_return = func(*args, **kwargs)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py\", line 1697, in init_process_group\n",
            "    raise ValueError(\n",
            "ValueError: device_id cuda:2 is out of range. Please use a device index less than the number of accelerators available: 1.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Time-LLM/run_main.py\", line 104, in <module>\n",
            "    accelerator = Accelerator(kwargs_handlers=[ddp_kwargs], deepspeed_plugin=deepspeed_plugin)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py\", line 461, in __init__\n",
            "    self.state = AcceleratorState(\n",
            "                 ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/state.py\", line 912, in __init__\n",
            "    PartialState(cpu, **kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/state.py\", line 216, in __init__\n",
            "    dist.init_distributed(dist_backend=self.backend, auto_mpi_discovery=False, **kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/deepspeed/comm/comm.py\", line 854, in init_distributed\n",
            "    cdb = TorchBackend(dist_backend, timeout, init_method, rank, world_size)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/deepspeed/comm/torch.py\", line 120, in __init__\n",
            "    self.init_process_group(backend, timeout, init_method, rank, world_size)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/deepspeed/comm/torch.py\", line 160, in init_process_group\n",
            "    torch.distributed.init_process_group(backend, **kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/c10d_logger.py\", line 81, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/c10d_logger.py\", line 95, in wrapper\n",
            "    func_return = func(*args, **kwargs)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py\", line 1697, in init_process_group\n",
            "    raise ValueError(\n",
            "ValueError: device_id cuda:4 is out of range. Please use a device index less than the number of accelerators available: 1.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Time-LLM/run_main.py\", line 104, in <module>\n",
            "    accelerator = Accelerator(kwargs_handlers=[ddp_kwargs], deepspeed_plugin=deepspeed_plugin)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py\", line 461, in __init__\n",
            "    self.state = AcceleratorState(\n",
            "                 ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/state.py\", line 912, in __init__\n",
            "    PartialState(cpu, **kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/state.py\", line 216, in __init__\n",
            "    dist.init_distributed(dist_backend=self.backend, auto_mpi_discovery=False, **kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/deepspeed/comm/comm.py\", line 854, in init_distributed\n",
            "    cdb = TorchBackend(dist_backend, timeout, init_method, rank, world_size)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/deepspeed/comm/torch.py\", line 120, in __init__\n",
            "    self.init_process_group(backend, timeout, init_method, rank, world_size)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/deepspeed/comm/torch.py\", line 160, in init_process_group\n",
            "    torch.distributed.init_process_group(backend, **kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/c10d_logger.py\", line 81, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/c10d_logger.py\", line 95, in wrapper\n",
            "    func_return = func(*args, **kwargs)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py\", line 1697, in init_process_group\n",
            "    raise ValueError(\n",
            "ValueError: device_id cuda:1 is out of range. Please use a device index less than the number of accelerators available: 1.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Time-LLM/run_main.py\", line 104, in <module>\n",
            "    accelerator = Accelerator(kwargs_handlers=[ddp_kwargs], deepspeed_plugin=deepspeed_plugin)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py\", line 461, in __init__\n",
            "    self.state = AcceleratorState(\n",
            "                 ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/state.py\", line 912, in __init__\n",
            "    PartialState(cpu, **kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/state.py\", line 216, in __init__\n",
            "    dist.init_distributed(dist_backend=self.backend, auto_mpi_discovery=False, **kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/deepspeed/comm/comm.py\", line 854, in init_distributed\n",
            "    cdb = TorchBackend(dist_backend, timeout, init_method, rank, world_size)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/deepspeed/comm/torch.py\", line 120, in __init__\n",
            "    self.init_process_group(backend, timeout, init_method, rank, world_size)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/deepspeed/comm/torch.py\", line 160, in init_process_group\n",
            "    torch.distributed.init_process_group(backend, **kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/c10d_logger.py\", line 81, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/c10d_logger.py\", line 95, in wrapper\n",
            "    func_return = func(*args, **kwargs)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py\", line 1697, in init_process_group\n",
            "    raise ValueError(\n",
            "ValueError: device_id cuda:7 is out of range. Please use a device index less than the number of accelerators available: 1.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Time-LLM/run_main.py\", line 104, in <module>\n",
            "    accelerator = Accelerator(kwargs_handlers=[ddp_kwargs], deepspeed_plugin=deepspeed_plugin)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py\", line 461, in __init__\n",
            "    self.state = AcceleratorState(\n",
            "                 ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/state.py\", line 912, in __init__\n",
            "    PartialState(cpu, **kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/state.py\", line 216, in __init__\n",
            "    dist.init_distributed(dist_backend=self.backend, auto_mpi_discovery=False, **kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/deepspeed/comm/comm.py\", line 854, in init_distributed\n",
            "    cdb = TorchBackend(dist_backend, timeout, init_method, rank, world_size)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/deepspeed/comm/torch.py\", line 120, in __init__\n",
            "    self.init_process_group(backend, timeout, init_method, rank, world_size)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/deepspeed/comm/torch.py\", line 160, in init_process_group\n",
            "    torch.distributed.init_process_group(backend, **kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/c10d_logger.py\", line 81, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/c10d_logger.py\", line 95, in wrapper\n",
            "    func_return = func(*args, **kwargs)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py\", line 1697, in init_process_group\n",
            "    raise ValueError(\n",
            "ValueError: device_id cuda:5 is out of range. Please use a device index less than the number of accelerators available: 1.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Time-LLM/run_main.py\", line 104, in <module>\n",
            "    accelerator = Accelerator(kwargs_handlers=[ddp_kwargs], deepspeed_plugin=deepspeed_plugin)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py\", line 461, in __init__\n",
            "    self.state = AcceleratorState(\n",
            "                 ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/state.py\", line 912, in __init__\n",
            "    PartialState(cpu, **kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/state.py\", line 216, in __init__\n",
            "    dist.init_distributed(dist_backend=self.backend, auto_mpi_discovery=False, **kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/deepspeed/comm/comm.py\", line 854, in init_distributed\n",
            "    cdb = TorchBackend(dist_backend, timeout, init_method, rank, world_size)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/deepspeed/comm/torch.py\", line 120, in __init__\n",
            "    self.init_process_group(backend, timeout, init_method, rank, world_size)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/deepspeed/comm/torch.py\", line 160, in init_process_group\n",
            "    torch.distributed.init_process_group(backend, **kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/c10d_logger.py\", line 81, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/c10d_logger.py\", line 95, in wrapper\n",
            "    func_return = func(*args, **kwargs)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py\", line 1697, in init_process_group\n",
            "    raise ValueError(\n",
            "ValueError: device_id cuda:6 is out of range. Please use a device index less than the number of accelerators available: 1.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Time-LLM/run_main.py\", line 104, in <module>\n",
            "    accelerator = Accelerator(kwargs_handlers=[ddp_kwargs], deepspeed_plugin=deepspeed_plugin)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py\", line 461, in __init__\n",
            "    self.state = AcceleratorState(\n",
            "                 ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/state.py\", line 912, in __init__\n",
            "    PartialState(cpu, **kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/state.py\", line 216, in __init__\n",
            "    dist.init_distributed(dist_backend=self.backend, auto_mpi_discovery=False, **kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/deepspeed/comm/comm.py\", line 854, in init_distributed\n",
            "    cdb = TorchBackend(dist_backend, timeout, init_method, rank, world_size)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/deepspeed/comm/torch.py\", line 120, in __init__\n",
            "    self.init_process_group(backend, timeout, init_method, rank, world_size)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/deepspeed/comm/torch.py\", line 160, in init_process_group\n",
            "    torch.distributed.init_process_group(backend, **kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/c10d_logger.py\", line 81, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/c10d_logger.py\", line 95, in wrapper\n",
            "    func_return = func(*args, **kwargs)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py\", line 1697, in init_process_group\n",
            "    raise ValueError(\n",
            "ValueError: device_id cuda:3 is out of range. Please use a device index less than the number of accelerators available: 1.\n",
            "W1116 12:16:33.558000 11239 torch/distributed/elastic/multiprocessing/api.py:900] Sending process 11348 closing signal SIGTERM\n",
            "W1116 12:16:33.559000 11239 torch/distributed/elastic/multiprocessing/api.py:900] Sending process 11349 closing signal SIGTERM\n",
            "W1116 12:16:33.560000 11239 torch/distributed/elastic/multiprocessing/api.py:900] Sending process 11350 closing signal SIGTERM\n",
            "W1116 12:16:33.563000 11239 torch/distributed/elastic/multiprocessing/api.py:900] Sending process 11351 closing signal SIGTERM\n",
            "W1116 12:16:33.564000 11239 torch/distributed/elastic/multiprocessing/api.py:900] Sending process 11353 closing signal SIGTERM\n",
            "W1116 12:16:33.565000 11239 torch/distributed/elastic/multiprocessing/api.py:900] Sending process 11354 closing signal SIGTERM\n",
            "W1116 12:16:33.572000 11239 torch/distributed/elastic/multiprocessing/api.py:900] Sending process 11355 closing signal SIGTERM\n",
            "E1116 12:16:33.790000 11239 torch/distributed/elastic/multiprocessing/api.py:874] failed (exitcode: 1) local_rank: 4 (pid: 11352) of binary: /usr/bin/python3\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/accelerate\", line 10, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/commands/accelerate_cli.py\", line 50, in main\n",
            "    args.func(args)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/commands/launch.py\", line 1226, in launch_command\n",
            "    multi_gpu_launcher(args)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/commands/launch.py\", line 853, in multi_gpu_launcher\n",
            "    distrib_run.run(args)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/run.py\", line 892, in run\n",
            "    elastic_launch(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/launcher/api.py\", line 143, in __call__\n",
            "    return launch_agent(self._config, self._entrypoint, list(args))\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/launcher/api.py\", line 277, in launch_agent\n",
            "    raise ChildFailedError(\n",
            "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
            "============================================================\n",
            "run_main.py FAILED\n",
            "------------------------------------------------------------\n",
            "Failures:\n",
            "  <NO_OTHER_FAILURES>\n",
            "------------------------------------------------------------\n",
            "Root Cause (first observed failure):\n",
            "[0]:\n",
            "  time      : 2025-11-16_12:16:33\n",
            "  host      : bb8905d3fe8a\n",
            "  rank      : 4 (local_rank: 4)\n",
            "  exitcode  : 1 (pid: 11352)\n",
            "  error_file: <N/A>\n",
            "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
            "============================================================\n",
            "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
            "\t`--num_machines` was set to a value of `1`\n",
            "\t`--dynamo_backend` was set to a value of `'no'`\n",
            "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
            "2025-11-16 12:16:57.476587: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-16 12:16:57.494000: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-11-16 12:16:57.503180: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763295417.517160   11971 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-11-16 12:16:57.519690: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "E0000 00:00:1763295417.523902   11971 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763295417.540104   11971 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763295417.540137   11971 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763295417.540140   11971 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763295417.540143   11971 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763295417.540535   11970 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-11-16 12:16:57.544847: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "E0000 00:00:1763295417.546811   11970 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763295417.562348   11970 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763295417.562381   11970 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763295417.562384   11970 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763295417.562386   11970 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-16 12:16:57.567041: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-11-16 12:16:57.726937: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-16 12:16:57.750480: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763295417.773989   11974 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763295417.780405   11974 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763295417.795929   11974 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763295417.795961   11974 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763295417.795964   11974 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763295417.795967   11974 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-16 12:16:57.800590: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-11-16 12:16:57.913099: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-16 12:16:57.930361: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763295417.952017   11968 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763295417.958501   11968 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763295417.974441   11968 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763295417.974474   11968 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763295417.974477   11968 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763295417.974480   11968 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-16 12:16:57.979329: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-11-16 12:17:00.041427: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-16 12:17:00.058686: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763295420.080370   11967 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763295420.086682   11967 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763295420.102124   11967 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763295420.102155   11967 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763295420.102158   11967 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763295420.102160   11967 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-16 12:17:00.106818: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-11-16 12:17:00.293996: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-16 12:17:00.319850: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763295420.353723   11973 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763295420.364112   11973 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763295420.388302   11973 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763295420.388345   11973 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763295420.388349   11973 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763295420.388353   11973 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-16 12:17:00.395266: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-11-16 12:17:00.544476: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-16 12:17:00.559381: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-11-16 12:17:00.571393: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-11-16 12:17:00.586832: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763295420.607118   11972 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763295420.617896   11972 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763295420.623868   11969 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763295420.634612   11969 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763295420.642939   11972 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763295420.642986   11972 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763295420.642992   11972 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763295420.642997   11972 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-16 12:17:00.650615: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "W0000 00:00:1763295420.659759   11969 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763295420.659809   11969 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763295420.659815   11969 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763295420.659819   11969 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-16 12:17:00.667053: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Time-LLM/run_main.py\", line 104, in <module>\n",
            "    accelerator = Accelerator(kwargs_handlers=[ddp_kwargs], deepspeed_plugin=deepspeed_plugin)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py\", line 461, in __init__\n",
            "    self.state = AcceleratorState(\n",
            "                 ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/state.py\", line 912, in __init__\n",
            "    PartialState(cpu, **kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/state.py\", line 216, in __init__\n",
            "    dist.init_distributed(dist_backend=self.backend, auto_mpi_discovery=False, **kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/deepspeed/comm/comm.py\", line 854, in init_distributed\n",
            "    cdb = TorchBackend(dist_backend, timeout, init_method, rank, world_size)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/deepspeed/comm/torch.py\", line 120, in __init__\n",
            "    self.init_process_group(backend, timeout, init_method, rank, world_size)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/deepspeed/comm/torch.py\", line 160, in init_process_group\n",
            "    torch.distributed.init_process_group(backend, **kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/c10d_logger.py\", line 81, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/c10d_logger.py\", line 95, in wrapper\n",
            "    func_return = func(*args, **kwargs)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py\", line 1697, in init_process_group\n",
            "    raise ValueError(\n",
            "ValueError: device_id cuda:7 is out of range. Please use a device index less than the number of accelerators available: 1.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Time-LLM/run_main.py\", line 104, in <module>\n",
            "    accelerator = Accelerator(kwargs_handlers=[ddp_kwargs], deepspeed_plugin=deepspeed_plugin)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py\", line 461, in __init__\n",
            "    self.state = AcceleratorState(\n",
            "                 ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/state.py\", line 912, in __init__\n",
            "    PartialState(cpu, **kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/state.py\", line 216, in __init__\n",
            "    dist.init_distributed(dist_backend=self.backend, auto_mpi_discovery=False, **kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/deepspeed/comm/comm.py\", line 854, in init_distributed\n",
            "    cdb = TorchBackend(dist_backend, timeout, init_method, rank, world_size)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/deepspeed/comm/torch.py\", line 120, in __init__\n",
            "    self.init_process_group(backend, timeout, init_method, rank, world_size)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/deepspeed/comm/torch.py\", line 160, in init_process_group\n",
            "    torch.distributed.init_process_group(backend, **kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/c10d_logger.py\", line 81, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/c10d_logger.py\", line 95, in wrapper\n",
            "    func_return = func(*args, **kwargs)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py\", line 1697, in init_process_group\n",
            "    raise ValueError(\n",
            "ValueError: device_id cuda:1 is out of range. Please use a device index less than the number of accelerators available: 1.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Time-LLM/run_main.py\", line 104, in <module>\n",
            "    accelerator = Accelerator(kwargs_handlers=[ddp_kwargs], deepspeed_plugin=deepspeed_plugin)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py\", line 461, in __init__\n",
            "    self.state = AcceleratorState(\n",
            "                 ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/state.py\", line 912, in __init__\n",
            "    PartialState(cpu, **kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/state.py\", line 216, in __init__\n",
            "    dist.init_distributed(dist_backend=self.backend, auto_mpi_discovery=False, **kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/deepspeed/comm/comm.py\", line 854, in init_distributed\n",
            "    cdb = TorchBackend(dist_backend, timeout, init_method, rank, world_size)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/deepspeed/comm/torch.py\", line 120, in __init__\n",
            "    self.init_process_group(backend, timeout, init_method, rank, world_size)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/deepspeed/comm/torch.py\", line 160, in init_process_group\n",
            "    torch.distributed.init_process_group(backend, **kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/c10d_logger.py\", line 81, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/c10d_logger.py\", line 95, in wrapper\n",
            "    func_return = func(*args, **kwargs)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py\", line 1697, in init_process_group\n",
            "    raise ValueError(\n",
            "ValueError: device_id cuda:4 is out of range. Please use a device index less than the number of accelerators available: 1.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Time-LLM/run_main.py\", line 104, in <module>\n",
            "    accelerator = Accelerator(kwargs_handlers=[ddp_kwargs], deepspeed_plugin=deepspeed_plugin)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py\", line 461, in __init__\n",
            "    self.state = AcceleratorState(\n",
            "                 ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/state.py\", line 912, in __init__\n",
            "    PartialState(cpu, **kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/state.py\", line 216, in __init__\n",
            "    dist.init_distributed(dist_backend=self.backend, auto_mpi_discovery=False, **kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/deepspeed/comm/comm.py\", line 854, in init_distributed\n",
            "    cdb = TorchBackend(dist_backend, timeout, init_method, rank, world_size)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/deepspeed/comm/torch.py\", line 120, in __init__\n",
            "    self.init_process_group(backend, timeout, init_method, rank, world_size)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/deepspeed/comm/torch.py\", line 160, in init_process_group\n",
            "    torch.distributed.init_process_group(backend, **kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/c10d_logger.py\", line 81, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/c10d_logger.py\", line 95, in wrapper\n",
            "    func_return = func(*args, **kwargs)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py\", line 1697, in init_process_group\n",
            "    raise ValueError(\n",
            "ValueError: device_id cuda:3 is out of range. Please use a device index less than the number of accelerators available: 1.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Time-LLM/run_main.py\", line 104, in <module>\n",
            "    accelerator = Accelerator(kwargs_handlers=[ddp_kwargs], deepspeed_plugin=deepspeed_plugin)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py\", line 461, in __init__\n",
            "    self.state = AcceleratorState(\n",
            "                 ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/state.py\", line 912, in __init__\n",
            "    PartialState(cpu, **kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/state.py\", line 216, in __init__\n",
            "    dist.init_distributed(dist_backend=self.backend, auto_mpi_discovery=False, **kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/deepspeed/comm/comm.py\", line 854, in init_distributed\n",
            "    cdb = TorchBackend(dist_backend, timeout, init_method, rank, world_size)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/deepspeed/comm/torch.py\", line 120, in __init__\n",
            "    self.init_process_group(backend, timeout, init_method, rank, world_size)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/deepspeed/comm/torch.py\", line 160, in init_process_group\n",
            "    torch.distributed.init_process_group(backend, **kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/c10d_logger.py\", line 81, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/c10d_logger.py\", line 95, in wrapper\n",
            "    func_return = func(*args, **kwargs)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py\", line 1697, in init_process_group\n",
            "    raise ValueError(\n",
            "ValueError: device_id cuda:5 is out of range. Please use a device index less than the number of accelerators available: 1.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Time-LLM/run_main.py\", line 104, in <module>\n",
            "    accelerator = Accelerator(kwargs_handlers=[ddp_kwargs], deepspeed_plugin=deepspeed_plugin)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py\", line 461, in __init__\n",
            "    self.state = AcceleratorState(\n",
            "                 ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/state.py\", line 912, in __init__\n",
            "    PartialState(cpu, **kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/state.py\", line 216, in __init__\n",
            "    dist.init_distributed(dist_backend=self.backend, auto_mpi_discovery=False, **kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/deepspeed/comm/comm.py\", line 854, in init_distributed\n",
            "    cdb = TorchBackend(dist_backend, timeout, init_method, rank, world_size)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/deepspeed/comm/torch.py\", line 120, in __init__\n",
            "    self.init_process_group(backend, timeout, init_method, rank, world_size)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/deepspeed/comm/torch.py\", line 160, in init_process_group\n",
            "    torch.distributed.init_process_group(backend, **kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/c10d_logger.py\", line 81, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/c10d_logger.py\", line 95, in wrapper\n",
            "    func_return = func(*args, **kwargs)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py\", line 1697, in init_process_group\n",
            "    raise ValueError(\n",
            "ValueError: device_id cuda:6 is out of range. Please use a device index less than the number of accelerators available: 1.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Time-LLM/run_main.py\", line 104, in <module>\n",
            "    accelerator = Accelerator(kwargs_handlers=[ddp_kwargs], deepspeed_plugin=deepspeed_plugin)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py\", line 461, in __init__\n",
            "    self.state = AcceleratorState(\n",
            "                 ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/state.py\", line 912, in __init__\n",
            "    PartialState(cpu, **kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/state.py\", line 216, in __init__\n",
            "    dist.init_distributed(dist_backend=self.backend, auto_mpi_discovery=False, **kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/deepspeed/comm/comm.py\", line 854, in init_distributed\n",
            "    cdb = TorchBackend(dist_backend, timeout, init_method, rank, world_size)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/deepspeed/comm/torch.py\", line 120, in __init__\n",
            "    self.init_process_group(backend, timeout, init_method, rank, world_size)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/deepspeed/comm/torch.py\", line 160, in init_process_group\n",
            "    torch.distributed.init_process_group(backend, **kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/c10d_logger.py\", line 81, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/c10d_logger.py\", line 95, in wrapper\n",
            "    func_return = func(*args, **kwargs)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py\", line 1697, in init_process_group\n",
            "    raise ValueError(\n",
            "ValueError: device_id cuda:2 is out of range. Please use a device index less than the number of accelerators available: 1.\n",
            "W1116 12:17:13.385000 11852 torch/distributed/elastic/multiprocessing/api.py:900] Sending process 11967 closing signal SIGTERM\n",
            "W1116 12:17:13.386000 11852 torch/distributed/elastic/multiprocessing/api.py:900] Sending process 11969 closing signal SIGTERM\n",
            "W1116 12:17:13.387000 11852 torch/distributed/elastic/multiprocessing/api.py:900] Sending process 11970 closing signal SIGTERM\n",
            "W1116 12:17:13.388000 11852 torch/distributed/elastic/multiprocessing/api.py:900] Sending process 11971 closing signal SIGTERM\n",
            "W1116 12:17:13.389000 11852 torch/distributed/elastic/multiprocessing/api.py:900] Sending process 11972 closing signal SIGTERM\n",
            "W1116 12:17:13.390000 11852 torch/distributed/elastic/multiprocessing/api.py:900] Sending process 11973 closing signal SIGTERM\n",
            "W1116 12:17:13.390000 11852 torch/distributed/elastic/multiprocessing/api.py:900] Sending process 11974 closing signal SIGTERM\n",
            "E1116 12:17:13.558000 11852 torch/distributed/elastic/multiprocessing/api.py:874] failed (exitcode: 1) local_rank: 1 (pid: 11968) of binary: /usr/bin/python3\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/accelerate\", line 10, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/commands/accelerate_cli.py\", line 50, in main\n",
            "    args.func(args)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/commands/launch.py\", line 1226, in launch_command\n",
            "    multi_gpu_launcher(args)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/commands/launch.py\", line 853, in multi_gpu_launcher\n",
            "    distrib_run.run(args)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/run.py\", line 892, in run\n",
            "    elastic_launch(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/launcher/api.py\", line 143, in __call__\n",
            "    return launch_agent(self._config, self._entrypoint, list(args))\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/launcher/api.py\", line 277, in launch_agent\n",
            "    raise ChildFailedError(\n",
            "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
            "============================================================\n",
            "run_main.py FAILED\n",
            "------------------------------------------------------------\n",
            "Failures:\n",
            "  <NO_OTHER_FAILURES>\n",
            "------------------------------------------------------------\n",
            "Root Cause (first observed failure):\n",
            "[0]:\n",
            "  time      : 2025-11-16_12:17:13\n",
            "  host      : bb8905d3fe8a\n",
            "  rank      : 1 (local_rank: 1)\n",
            "  exitcode  : 1 (pid: 11968)\n",
            "  error_file: <N/A>\n",
            "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "waqPX0eya9xa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}